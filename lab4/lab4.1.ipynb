{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb07df3a",
   "metadata": {},
   "source": [
    "### Lab 4 Task 1\n",
    "#### Обраний текст: https://www.kaggle.com/datasets/mykras/ukrainian-texts, Лис Микита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5403fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Файл Lys_mykyta.txt завантажено. Довжина тексту - 86526 символів\n",
      "Vocab size: 5000\n",
      "Parameters: 779528\n",
      "Початок навчання\n",
      "Epoch 10 | Loss: 0.3860\n",
      "Epoch 20 | Loss: 0.3097\n",
      "Epoch 30 | Loss: 0.2632\n",
      "\n",
      "Приклади генерації\n",
      "1 Temp 0.5: лис микита біг микита біг місце місце біг місце біг біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг місце фельдшериці біг\n",
      "2 Temp 0.7: цар лев цар лев цар цар цар цар цар цар цар цар цар й цар цар цар, то я не раз журився! ой мій зайку тут наплели. ну ж твої слова так сподобався попові мало і шахрай та гидкі ходить перед входом під стіною притуливсь? як пишавсь з\n",
      "3 Temp 1.0: ті безхвості ті став знаменита мужики наказ хвоста урвав княгиня аби бідний батько призьбі хрюка забутий. ну, а як чкурне серед поля! і шахрай бабай журливо в руках тримає золотій най з тобою рушу на втіху хтось охріма розбудив? хто гордий день ж то й незчувся ви зібрались\n",
      "4 Temp 1.2: вийшли в поле пузо погулять їх звідти старі між нами отак, праця лісочком страшного злість! як пишавсь торг собі у писанні тім вибігає. а це я пропащий тихо ж ото степами йшли хтось лізь з верхів'я помалесеньку злетів буде всім звірам був чудесний, мій одержавши без упину жінка там\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Фіксація випадкових чисел для відтворюваності\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Завантаження тексту з файлу\n",
    "filename = 'Lys_mykyta.txt'\n",
    "try:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text_raw = f.read()\n",
    "    print(f\"Файл {filename} завантажено. Довжина тексту - {len(text_raw)} символів\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Файл {filename} не знайдено.\")\n",
    "    text_raw = \"\" \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Залишаємо букви, цифри, апостроф, дефіс та пунктуацію\n",
    "    # \\u2019 - це типографський апостроф, ' - звичайний\n",
    "    text = re.sub(r\"[^а-яіїєґa-z0-9\\s\\-\\.,!?'\\u2019]\", \"\", text) \n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "corpus = clean_text(text_raw)\n",
    "\n",
    "# Токенізація на слова та знаки пунктуації\n",
    "def tokenize(text):\n",
    "    # Цей regex шукає:\n",
    "    # 1. Слова з апострофами та дефісами (м'ясо, по-нашому)\n",
    "    # 2. АБО окремі розділові знаки\n",
    "    pattern = r\"[а-яіїєґa-z0-9]+(?:['\\u2019\\-][а-яіїєґa-z0-9]+)*|[.,!?;]\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "tokens = tokenize(corpus)\n",
    "\n",
    "MAX_VOCAB = 5000\n",
    "specials = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    "\n",
    "counter = Counter(tokens)\n",
    "most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
    "itos = specials + [w for w, _ in most_common]\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "PAD_IDX = stoi[\"[PAD]\"]\n",
    "UNK_IDX = stoi[\"[UNK]\"]\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "BLOCK_SIZE = 16\n",
    "\n",
    "# Підготовка даних для навчання\n",
    "class TextGenerationDataset(Dataset):\n",
    "    def __init__(self, tokens, stoi, block_size):\n",
    "        self.data = [stoi.get(t, UNK_IDX) for t in tokens]\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx + self.block_size + 1]\n",
    "        chunk_tensor = torch.tensor(chunk, dtype=torch.long)\n",
    "        return chunk_tensor[:-1], chunk_tensor[1:]\n",
    "\n",
    "dataset = TextGenerationDataset(tokens, stoi, BLOCK_SIZE)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# Створення маски щоб приховати майбутні слова\n",
    "def generate_subsequent_mask(size, device):\n",
    "    mask = torch.triu(torch.ones(size, size, dtype=torch.bool, device=device), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, n_heads=4, num_layers=2, d_ff=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenPositionalEmbedding(vocab_size, d_model, max_len=BLOCK_SIZE, dropout=dropout)\n",
    "        \n",
    "        dec_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=d_ff, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.embedding(src)\n",
    "        \n",
    "        T = src.size(1)\n",
    "        mask = generate_subsequent_mask(T, src.device)\n",
    "        \n",
    "        out = self.decoder(\n",
    "            tgt=x, \n",
    "            memory=x, \n",
    "            tgt_mask=mask\n",
    "        )\n",
    "        \n",
    "        logits = self.output_proj(out)\n",
    "        return logits\n",
    "\n",
    "model = TransformerGenerator(vocab_size=vocab_size).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Навчання моделі\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 30 \n",
    "print(\"Початок навчання\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        \n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B*T, V), y.view(B*T))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Функція генерації тексту\n",
    "@torch.no_grad()\n",
    "def generate_text(model, start_text, max_new_tokens=50, temperature=1.0, top_k=10, repetition_penalty=1.2):\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenize(clean_text(start_text))\n",
    "    idx = [stoi.get(t, UNK_IDX) for t in tokens]\n",
    "    \n",
    "    # Захист від порожнього входу\n",
    "    if not idx:\n",
    "        idx = [stoi.get(tokens[0], UNK_IDX)] if tokens else [UNK_IDX]\n",
    "        \n",
    "    input_tensor = torch.tensor(idx, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    result_idx = idx[:]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        cond = input_tensor[:, -BLOCK_SIZE:]\n",
    "        \n",
    "        logits = model(cond)\n",
    "        last_logits = logits[:, -1, :] \n",
    "        \n",
    "        for token_id in set(result_idx):\n",
    "            if last_logits[0, token_id] < 0:\n",
    "                last_logits[0, token_id] *= repetition_penalty\n",
    "            else:\n",
    "                last_logits[0, token_id] /= repetition_penalty\n",
    "\n",
    "        last_logits = last_logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(last_logits, top_k)\n",
    "            last_logits[last_logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "        \n",
    "        # Випадковий вибір зваженого токена\n",
    "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        result_idx.append(next_idx)\n",
    "        input_tensor = torch.cat((input_tensor, torch.tensor([[next_idx]], device=device)), dim=1)\n",
    "\n",
    "        if next_idx == stoi.get(\"[EOS]\", -1):\n",
    "            break\n",
    "\n",
    "    decoded = [itos[i] for i in result_idx]\n",
    "    output_text = \" \".join(decoded)\n",
    "    output_text = re.sub(r'\\s([.,!?;])', r'\\1', output_text)\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "print(\"\\nПриклади генерації\")\n",
    "print(\"1 Temp 0.5:\", generate_text(model, \"Лис Микита біг\", temperature=0.5, repetition_penalty=1))\n",
    "print(\"2 Temp 0.7:\", generate_text(model, \"Цар Лев\", temperature=0.7, repetition_penalty=1.5))\n",
    "print(\"3 Temp 1.0:\", generate_text(model, \"Ті безхвості\", temperature=1.0, repetition_penalty=1.7))\n",
    "print(\"4 Temp 1.2:\", generate_text(model, \"Вийшли в поле\", temperature=1.2, repetition_penalty=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
